{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector AI coding Challenge\n",
    "## Alberto Raimondi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install fuzzywuzzy\n",
    "# !pip3 install python-Levenshtein\n",
    "# !pip3 install cleanco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insatlling spacy model for entity recognition\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alberto/misc/anaconda3/lib/python3.7/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "import usaddress\n",
    "import string\n",
    "\n",
    "from collections import Counter, defaultdict, OrderedDict\n",
    "import pycountry\n",
    "from fuzzywuzzy import fuzz, process\n",
    "from cleanco import prepare_terms, basename\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "company_terms = prepare_terms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first task consists of creating a set of functions able to identify duplicate entities for specific kinds of inputs.\n",
    "The data given is very small so the training of a full machine learning model is not the best way to proceed, we will, however, rely on a set of hard-coded rules and heuristics to find matching items.\n",
    "The 5 categories are the following:\n",
    "- companies\n",
    "- addresses\n",
    "- serial numbers\n",
    "- item names\n",
    "- locations\n",
    "\n",
    "for each of them, we will implement a similar approach with few relevant differences bases on the category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies = [\n",
    "    \"Marks and Spencers Ltd\",\n",
    "    \"M&S Limited\",\n",
    "    \"NVIDIA Ireland\",\n",
    "]\n",
    "\n",
    "addresses = [\n",
    "    \"SLOUGH SE12 2XY\",\n",
    "    \"33 TIMBER YARD, LONDON, L1 8XY\",\n",
    "    \"44 CHINA ROAD, KOWLOON, HONG KONG\",\n",
    "]\n",
    "serials = [\n",
    "    \"XYZ 13423 / ILD\",\n",
    "    \"ABC/ICL/20891NC\",\n",
    "]\n",
    "items = [\n",
    "    \"HARDWOOD TABLE\",\n",
    "    \"PLASTIC BOTTLE\",\n",
    "]\n",
    "\n",
    "locations = [\n",
    "    \"LONDON\",\n",
    "    \"HONG KONG\",\n",
    "    \"ASIA\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Companies\n",
    "\n",
    "We will assume that the corporation type of a company is irrelevant for its classification, this will allow the use of fuzzy matching techniques that would fail otherwise.\n",
    "We also assume that if the location of the company is specified in its name it will be relevant for its classification. E.g. Nvidia Ireland will be a different entity than Nvidia Germany.\n",
    "\n",
    "The following preprocessing steps are taken:\n",
    "- substitution of the common ampersand character with the word \"and\"\n",
    "- removal of punctuation\n",
    "- making the strings lowercase\n",
    "- removal of corporation type acronyms\n",
    "\n",
    "The removal of the company-type acronyms could lead to some relevant information being lost by the system, it is however the best way to solve this task with this amount of training examples while avoiding conflicts in matching.\n",
    "\n",
    "The deduplication of the entities is then done by using a fuzzy matcher allowing the matching of acronyms and partial words.\n",
    "\n",
    "In a more complex system, usage of a trained knowledge-based would be much more effective, however, due to the low volume of the data this manual approach will be more effective for now. For future implementations, the SpaCy entity linking module is very effective given a relevant knowledge base.\n",
    "\n",
    "An interesting problem is the use of acronyms in documents, this is tricky to deal with due to the possible use of the same acronym for many words. For example, both \"Marks and Spencers Ltd\" and \"Mason and Spears L.T.D.\" could be mapped to \"M&S Limited\". In a real-world situation, this problem could probably be ameliorated by using a heuristic based on the context of the document. If a company is mentioned in the document and then its acronym is used then we could be fairly sure that they are a match. At the same time if the acronym never appeared in any of the previous documents would be safe to assume for it to be a new entity. These heuristics should also account for the popularity of some acronyms, AMD ltd is a fairly popular company and a match in a knowledge base could be used to identify it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change ampersand\n",
    "companies = [c.translate(str.maketrans({\"&\": \" and \"})) for c in companies]\n",
    "# make lowercase\n",
    "companies = [c.lower() for c in companies]\n",
    "# remove punctuation\n",
    "companies = [c.translate(str.maketrans(\"\", \"\", string.punctuation)) for c in companies]\n",
    "# company type normalization\n",
    "companies = [\n",
    "    basename(c, company_terms, prefix=True, middle=False, suffix=True)\n",
    "    for c in companies\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n"
     ]
    }
   ],
   "source": [
    "print(fuzz.token_set_ratio(companies[0], companies[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['marks and spencers', 'nvidia ireland'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process.dedupe(companies, threshold=59, scorer=fuzz.token_set_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serial numbers\n",
    "\n",
    "The case of serial numbers is an easier one, they are extremely specific and we can be safe to assume they map the products in a non-continuous way (a small change in the serial number may mean a different product). They are also very sensitive to the position of the characters and typo detection should be avoided.\n",
    "We can also assume that the punctuation and the spacing in a serial number are only for readability reasons, allowing us to strip it. For these reasons will use a much more strict edit distance to compare serial numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make lowercase\n",
    "serials = [c.lower() for c in serials]\n",
    "# remove punctuation and spaces\n",
    "serials = [\n",
    "    c.translate(str.maketrans(\"\", \"\", string.punctuation + \" \")) for c in serials\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xyz13423ild', 'abcicl20891nc']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use stricter Levensthein distance\n",
    "process.dedupe(serials, threshold=90, scorer=fuzz.ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Items\n",
    "\n",
    "The process for items will be similar to the previous ones, the order of the words in the strings should not matter much but the string themselves should. A fuzzy matching ignoring string order should work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = [c.lower() for c in items]\n",
    "items = [\n",
    "    c.translate(str.maketrans(\"\", \"\", string.punctuation)) for c in items\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hardwood table', 'plastic bottle']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process.dedupe(items, threshold=80, scorer=fuzz.token_set_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addresses \n",
    "The normalization of addresses would straightforward by using the google maps API, there is a limited number of unique locations and the Google Maps API has a fully trained disambiguation model that works well.\n",
    "An alternative way to do this while avoiding the costs of the API requests would be to use the OpenStreetMap data that is freely available and easy to parse locally, at the cost of a higher computational expense.\n",
    "This approach however brings some edge cases that should be dealt with. Many addresses do not have a unique match if not fully specified and this could lead to bad performance and mismatches. \n",
    "A good way to solve this would be to compare the distance of the possible matches to the known location where a company operates in a knowledge base, the closest address would be the most probable match. Also asking for user input for disambiguation would be a good solution.\n",
    "\n",
    "Due to the cost of the Google Maps API and the need for much more data to be able to parse all types of addresses we will rely on an external library to perform a much simpler address parsing to normalize the order of the elements and then use a fuzzy matcher again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make lowercase\n",
    "addresses = [c.lower() for c in addresses]\n",
    "# # remove punctuation\n",
    "addresses = [\n",
    "    c.translate(str.maketrans(\" \", \" \", string.punctuation)) for c in addresses\n",
    "]\n",
    "# tag the parts of the address and sort them alphabetically to make them have the same order\n",
    "for a in addresses:\n",
    "    parsed_address = usaddress.tag(a)\n",
    "    elements = parsed_address[0]\n",
    "    elements = OrderedDict(sorted(elements.items(), key=lambda item: item[0]))\n",
    "\n",
    "    a = \" \".join(elements.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['slough se12 2xy',\n",
       " '33 timber yard london l1 8xy',\n",
       " '44 china road kowloon hong kong']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process.dedupe(addresses, threshold=70, scorer=fuzz.ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Locations\n",
    "Locations are probably the most difficult items to deal with, many libraries are available but few of them are good at getting a good match. A fuzzy matcher is used again but many improvements could be achieved by having more data to evaluate what is the possible value of the field to manually craft a better parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make lowercase\n",
    "locations = [c.lower() for c in locations]\n",
    "# # remove punctuation\n",
    "locations = [c.translate(str.maketrans(\"\", \"\", string.punctuation)) for c in locations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['london', 'hong kong', 'asia']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process.dedupe(locations, threshold=70, scorer=fuzz.ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second task is very different from the first one, we need to match the already seen entities but we can only access a continuous stream of them and we do not have access to their category beforehand.\n",
    "The unavailability of the category makes the use of previously devised techniques much harder because we cannot directly apply the pipeline for the right class.\n",
    "\n",
    "For this reason, a good solution would be to create a classifier able to distinguish the category of the input.\n",
    "After testing various approaches based on the SpaCy NER module and the GloVe word embeddings we struggled to find a model effective for the low amount of data in this task.\n",
    "\n",
    "Without access to the class of the input a different pipeline had to be built for this task, the only classification we did is to find if the input is an address as the probability of having conflicts between locations and addresses is very high. By separating the pipeline for addresses and locations we can use a fuzzy matching on the items on the assumption that the content of the various categories is different enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Marks and Spencers Ltd',\n",
       " 'M&S Limited',\n",
       " 'NVIDIA Ireland',\n",
       " 'INTEL LLC',\n",
       " 'XYZ 13423 / ILD',\n",
       " 'ABC/ICL/20891NC',\n",
       " 'ICNAO02312',\n",
       " 'LONDON',\n",
       " 'LONDON, ENGLAND',\n",
       " 'HONG KONG',\n",
       " 'LONDON, GREAT BRITAIN',\n",
       " 'ASIA',\n",
       " 'HARDWOOD TABLE',\n",
       " 'PLASTIC BOTTLE',\n",
       " 'TOYS',\n",
       " 'SLOUGH SE12 2XY',\n",
       " '33 TIMBER YARD, LONDON, L1 8XY',\n",
       " '44 CHINA ROAD, KOWLOON, HONG KONG']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "companies = [\n",
    "    \"Marks and Spencers Ltd\",\n",
    "    \"M&S Limited\",\n",
    "    \"NVIDIA Ireland\",\n",
    "    \"INTEL LLC\",\n",
    "]\n",
    "\n",
    "addresses = [\n",
    "    \"SLOUGH SE12 2XY\",\n",
    "    \"33 TIMBER YARD, LONDON, L1 8XY\",\n",
    "    \"44 CHINA ROAD, KOWLOON, HONG KONG\",\n",
    "]\n",
    "serials = [\n",
    "    \"XYZ 13423 / ILD\",\n",
    "    \"ABC/ICL/20891NC\",\n",
    "    \"ICNAO02312\",\n",
    "]\n",
    "items = [\n",
    "    \"HARDWOOD TABLE\",\n",
    "    \"PLASTIC BOTTLE\",\n",
    "    \"TOYS\",\n",
    "]\n",
    "\n",
    "locations = [\n",
    "    \"LONDON\",\n",
    "    \"LONDON, ENGLAND\",\n",
    "    \"HONG KONG\",\n",
    "    \"LONDON, GREAT BRITAIN\",\n",
    "    \"ASIA\",\n",
    "]\n",
    "#%%\n",
    "stream = [i for c in [companies, serials, locations, items, addresses] for i in c]\n",
    "stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Marks', 'ORG')]\n",
      "[('M&S Limited', 'ORG')]\n",
      "[]\n",
      "[('INTEL', 'ORG')]\n",
      "[('XYZ 13423 / ILD', 'ORG')]\n",
      "[('ABC', 'ORG')]\n",
      "[]\n",
      "[('LONDON', 'GPE')]\n",
      "[('LONDON', 'GPE'), ('ENGLAND', 'PERSON')]\n",
      "[('HONG KONG', 'GPE')]\n",
      "[('LONDON', 'GPE'), ('GREAT BRITAIN', 'FAC')]\n",
      "[('ASIA', 'LOC')]\n",
      "[('HARDWOOD TABLE', 'ORG')]\n",
      "[]\n",
      "[]\n",
      "[('SLOUGH', 'ORG')]\n",
      "[('33', 'CARDINAL'), ('LONDON', 'GPE'), ('8XY', 'CARDINAL')]\n",
      "[('44', 'CARDINAL'), ('KOWLOON', 'ORG'), ('HONG KONG', 'GPE')]\n"
     ]
    }
   ],
   "source": [
    "# testing SpaCy classifier\n",
    "for item in stream:\n",
    "    doc = nlp(item)\n",
    "    ents = [(e.text, e.label_) for e in doc.ents]\n",
    "    print(ents) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreamProcessor:\n",
    "    def __init__(self, threshold=60):\n",
    "        self.company_terms = prepare_terms()\n",
    "        self.entities = defaultdict(list)\n",
    "        #use different thresholds based on the category\n",
    "        self.thresholds = {\"address\": 80, \"other\": 59}\n",
    "\n",
    "    def process_element(self, item):\n",
    "        item_class = \"other\"\n",
    "        try:\n",
    "            parsed_address = usaddress.tag(item)\n",
    "        except usaddress.RepeatedLabelError:\n",
    "            parsed_address = (None, None)\n",
    "        # if the item is an address we remove the name of the city to avoid conflicts\n",
    "        if parsed_address[1] == \"Street Address\":\n",
    "            parsed_address[0].pop(\"PlaceName\")\n",
    "            item = \" \".join(parsed_address[0].values())\n",
    "            \n",
    "            item_class == \"address\"\n",
    "        for entity in self.entities.keys():\n",
    "            for alias in self.entities[entity]:\n",
    "                # if the current element matches any of the already known aliases save it as a new alias\n",
    "                if fuzz.token_set_ratio(self.preprocess(alias), self.preprocess(item))>= self.thresholds[item_class]:\n",
    "                    self.entities[entity].append(item)\n",
    "                    return True\n",
    "        # if not create a new entity using the original string and add itself as a alias\n",
    "        self.entities[item].append(item)\n",
    "\n",
    "    def preprocess(self, item):\n",
    "        # change ampersand\n",
    "        item = item.translate(str.maketrans({\"&\": \" and \"}))\n",
    "        # make lowercase\n",
    "        item = item.lower()\n",
    "        # company type normalization\n",
    "        item = basename(item, company_terms, prefix=True, middle=False, suffix=True)\n",
    "        # remove punctuation\n",
    "        item = item.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = StreamProcessor()\n",
    "\n",
    "for s in stream:\n",
    "    processor.process_element(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'Marks and Spencers Ltd': ['Marks and Spencers Ltd',\n",
       "              'M&S Limited'],\n",
       "             'NVIDIA Ireland': ['NVIDIA Ireland'],\n",
       "             'INTEL LLC': ['INTEL LLC'],\n",
       "             'XYZ 13423 / ILD': ['XYZ 13423 / ILD'],\n",
       "             'ABC/ICL/20891NC': ['ABC/ICL/20891NC'],\n",
       "             'ICNAO02312': ['ICNAO02312'],\n",
       "             'LONDON': ['LONDON', 'LONDON, ENGLAND', 'LONDON, GREAT BRITAIN'],\n",
       "             'HONG KONG': ['HONG KONG'],\n",
       "             'ASIA': ['ASIA'],\n",
       "             'HARDWOOD TABLE': ['HARDWOOD TABLE'],\n",
       "             'PLASTIC BOTTLE': ['PLASTIC BOTTLE'],\n",
       "             'TOYS': ['TOYS'],\n",
       "             'SLOUGH SE12 2XY': ['SLOUGH SE12 2XY'],\n",
       "             '33 TIMBER YARD L1 8XY': ['33 TIMBER YARD L1 8XY'],\n",
       "             '44 CHINA ROAD': ['44 CHINA ROAD']})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "All of these models are prototypes only, a much more in-depth study should be done to see the characteristics of the data and their distribution so that more effective parsing techniques can be developed.\n",
    "\n",
    "The most interesting avenue for this task is the linking to a knowledge-base trained on past data and able to use information about the final use for the information. Given the focus on document parsing of Vector, an interesting avenue would be the building of a knowledge base of the clients and their interactions with location items and other entities. In this way, the matching of already seen entities would be much more accurate and effective."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
